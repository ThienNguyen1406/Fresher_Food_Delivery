"""
Product Ingest Pipeline - X·ª≠ l√Ω product v√† l∆∞u v√†o vector store theo category
Pipeline: Product (Text + Image) ‚Üí Embeddings ‚Üí Vector Database (theo category)
"""
import logging
import uuid
from datetime import datetime
from typing import Optional, List, Dict

from app.services.product import ProductEmbeddingService
from app.infrastructure.vector_store.image_vector_store import ImageVectorStore
from app.domain.document import DocumentChunk

logger = logging.getLogger(__name__)


class ProductIngestPipeline:
    """
    Pipeline x·ª≠ l√Ω product v√† l∆∞u v√†o vector store theo category
    
    Quy tr√¨nh:
    1. Nh·∫≠n product data (text + image)
    2. T·∫°o embeddings: image, text, combined
    3. L∆∞u v√†o Vector Database v·ªõi metadata: category_id, product_id
    """
    
    def __init__(
        self,
        product_embedding_service: ProductEmbeddingService,
        vector_store: ImageVectorStore
    ):
        """
        Kh·ªüi t·∫°o Product Ingest Pipeline
        
        Args:
            product_embedding_service: Service t·∫°o embeddings cho product
            vector_store: Vector store ƒë·ªÉ l∆∞u embeddings
        """
        self.product_embedding_service = product_embedding_service
        self.vector_store = vector_store
    
    async def process_and_store(
        self,
        product_data: Dict,
        image_bytes: Optional[bytes] = None,
        product_id: Optional[str] = None
    ) -> str:
        """
        X·ª≠ l√Ω product v√† l∆∞u v√†o vector store
        
        Args:
            product_data: Dict ch·ª©a th√¥ng tin product
                - product_name: T√™n s·∫£n ph·∫©m
                - description: M√¥ t·∫£
                - category_id: ID category
                - category_name: T√™n category
                - price: Gi√°
                - etc.
            image_bytes: ·∫¢nh s·∫£n ph·∫©m (t√πy ch·ªçn)
            product_id: ID s·∫£n ph·∫©m (t√πy ch·ªçn, s·∫Ω l·∫•y t·ª´ product_data n·∫øu c√≥)
            
        Returns:
            product_id c·ªßa s·∫£n ph·∫©m ƒë√£ x·ª≠ l√Ω
        """
        # L·∫•y product_id
        if not product_id:
            product_id = product_data.get('product_id') or f"PROD-{str(uuid.uuid4())[:8]}"
        
        category_id = product_data.get('category_id', '')
        category_name = product_data.get('category_name', '')
        product_name = product_data.get('product_name', '')
        
        try:
            logger.info(f"üõçÔ∏è  B·∫Øt ƒë·∫ßu x·ª≠ l√Ω product: {product_name} (ID: {product_id}, Category: {category_id})")
            
            # üî• T·ªêI ∆ØU: B∆∞·ªõc 1 - T·∫°o embeddings cho product
            # ProductEmbeddingService ƒë√£ tr·∫£ primary_embedding ƒë√£ normalize + combine
            # Pipeline KH√îNG g·ªçi model n·ªØa, ch·ªâ d√πng k·∫øt qu·∫£
            logger.info(f"üî¢ ƒêang t·∫°o embeddings cho product...")
            embeddings = await self.product_embedding_service.create_product_embeddings(
                product_data,
                image_bytes
            )
            
            # üî• L·∫•y primary_embedding ƒë√£ ƒë∆∞·ª£c normalize + combine (70% text CLIP + 30% image)
            primary_embedding = embeddings.get('primary_embedding')
            
            if primary_embedding is None:
                raise ValueError("Kh√¥ng th·ªÉ t·∫°o embedding cho product")
            
            logger.info(f"‚úÖ ƒê√£ t·∫°o primary embedding (dimension: {len(primary_embedding)})")
            
            # B∆∞·ªõc 2: T·∫°o DocumentChunk t·ª´ product
            chunk = DocumentChunk(
                chunk_id=f"{product_id}-chunk-0",
                file_id=product_id,
                file_name=product_name or f"Product_{product_id}",
                text=f"[Product: {product_name}] {product_data.get('description', '')}",
                chunk_index=0,
                start_index=0,
                end_index=0
            )
            
            # B∆∞·ªõc 3: L∆∞u v√†o vector store v·ªõi metadata ƒë·∫ßy ƒë·ªß
            logger.info(f"üíæ ƒêang l∆∞u product v√†o vector store...")
            upload_date = datetime.now().isoformat()
            
            # L·∫•y image filename t·ª´ product_data n·∫øu c√≥ (t·ª´ database khi embed)
            image_filename = product_data.get('image_filename') or product_data.get('anh')
            
            # üî• T·ªêI ∆ØU: Metadata ch·ªâ ch·ª©a filter keys, kh√¥ng ch·ª©a content
            # product_name, description ‚Üí l·∫•y t·ª´ SQL khi tr·∫£ k·∫øt qu·∫£
            # Gi·∫£m RAM vector store, th·ªùi gian serialize/deserialize, query latency
            price_value = product_data.get('price', '')
            if price_value:
                try:
                    price_float = float(price_value)
                except (ValueError, TypeError):
                    price_float = 0.0
            else:
                price_float = 0.0
            
            extra_metadata = [{
                "product_id": product_id,
                "category_id": category_id,
                "content_type": "product",
                "price": price_float,
                "has_image": bool(image_filename),  # Ch·ªâ l∆∞u boolean, kh√¥ng l∆∞u filename
            }]
            
            await self.vector_store.save_chunks(
                [chunk],
                [primary_embedding],
                file_type="product",
                upload_date=upload_date,
                extra_metadata=extra_metadata
            )
            
            logger.info(f"‚úÖ ƒê√£ x·ª≠ l√Ω v√† l∆∞u th√†nh c√¥ng product {product_name} (Category: {category_id})")
            
            return product_id
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói khi x·ª≠ l√Ω product {product_name}: {str(e)}", exc_info=True)
            raise
    
    async def process_and_store_batch(
        self,
        products: List[Dict],
        images: Optional[List[bytes]] = None
    ) -> List[str]:
        """
        X·ª≠ l√Ω nhi·ªÅu products c√πng l√∫c (batch)
        
        Args:
            products: Danh s√°ch product data
            images: Danh s√°ch ·∫£nh t∆∞∆°ng ·ª©ng (t√πy ch·ªçn)
            
        Returns:
            Danh s√°ch product_id ƒë√£ x·ª≠ l√Ω
        """
        if not products:
            return []
        
        try:
            logger.info(f"üõçÔ∏è  B·∫Øt ƒë·∫ßu x·ª≠ l√Ω batch {len(products)} products...")
            
            product_ids = []
            chunks = []
            embeddings_list = []
            upload_date = datetime.now().isoformat()
            
            # üî• T·ªêI ∆ØU: Batch embedding th·∫≠t - kh√¥ng loop t·ª´ng product
            # T·∫°o embeddings cho t·∫•t c·∫£ products c√πng l√∫c
            image_list = []
            for i in range(len(products)):
                if images and i < len(images):
                    image_list.append(images[i])
                else:
                    image_list.append(None)
            
            # Batch embed t·∫•t c·∫£ products
            embeddings_batch = await self.product_embedding_service.create_embeddings_batch(
                products,
                image_list
            )
            
            # T·∫°o chunks v√† l·∫•y primary_embeddings
            for i, product_data in enumerate(products):
                product_id = product_data.get('product_id') or f"PROD-{str(uuid.uuid4())[:8]}"
                embeddings = embeddings_batch[i] if i < len(embeddings_batch) else {}
                
                primary_embedding = embeddings.get('primary_embedding')
                if primary_embedding is None:
                    logger.warning(f"Skipping product {product_id}: kh√¥ng c√≥ embedding")
                    continue
                
                # T·∫°o chunk
                product_name = product_data.get('product_name', '')
                chunk = DocumentChunk(
                    chunk_id=f"{product_id}-chunk-0",
                    file_id=product_id,
                    file_name=product_name or f"Product_{product_id}",
                    text=f"[Product: {product_name}] {product_data.get('description', '')}",
                    chunk_index=0,
                    start_index=0,
                    end_index=0
                )
                
                chunks.append(chunk)
                embeddings_list.append(primary_embedding)
                product_ids.append(product_id)
            
            # L∆∞u t·∫•t c·∫£ c√πng l√∫c
            if chunks:
                # üî• T·ªêI ∆ØU: Metadata ch·ªâ ch·ª©a filter keys
                extra_metadata_list = []
                for i, product_data in enumerate(products):
                    if i < len(product_ids):
                        price_value = product_data.get('price', '')
                        try:
                            price_float = float(price_value) if price_value else 0.0
                        except (ValueError, TypeError):
                            price_float = 0.0
                        
                        image_filename = product_data.get('image_filename') or product_data.get('anh')
                        extra_metadata_list.append({
                            "product_id": product_ids[i],
                            "category_id": product_data.get('category_id', ''),
                            "content_type": "product",
                            "price": price_float,
                            "has_image": bool(image_filename),
                        })
                
                await self.vector_store.save_chunks(
                    chunks,
                    embeddings_list,
                    file_type="product",
                    upload_date=upload_date,
                    extra_metadata=extra_metadata_list
                )
            
            logger.info(f"‚úÖ ƒê√£ x·ª≠ l√Ω v√† l∆∞u th√†nh c√¥ng {len(product_ids)} products")
            
            return product_ids
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói khi x·ª≠ l√Ω batch products: {str(e)}", exc_info=True)
            raise
    

