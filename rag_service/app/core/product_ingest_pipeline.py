"""
Product Ingest Pipeline - X·ª≠ l√Ω product v√† l∆∞u v√†o vector store theo category
Pipeline: Product (Text + Image) ‚Üí Embeddings ‚Üí Vector Database (theo category)
"""
import logging
import uuid
from datetime import datetime
from typing import Optional, List, Dict
import numpy as np

from app.services.product import ProductEmbeddingService
from app.infrastructure.vector_store.image_vector_store import ImageVectorStore
from app.domain.document import DocumentChunk

logger = logging.getLogger(__name__)


class ProductIngestPipeline:
    """
    Pipeline x·ª≠ l√Ω product v√† l∆∞u v√†o vector store theo category
    
    Quy tr√¨nh:
    1. Nh·∫≠n product data (text + image)
    2. T·∫°o embeddings: image, text, combined
    3. L∆∞u v√†o Vector Database v·ªõi metadata: category_id, product_id
    """
    
    def __init__(
        self,
        product_embedding_service: ProductEmbeddingService,
        vector_store: ImageVectorStore
    ):
        """
        Kh·ªüi t·∫°o Product Ingest Pipeline
        
        Args:
            product_embedding_service: Service t·∫°o embeddings cho product
            vector_store: Vector store ƒë·ªÉ l∆∞u embeddings
        """
        self.product_embedding_service = product_embedding_service
        self.vector_store = vector_store
    
    async def process_and_store(
        self,
        product_data: Dict,
        image_bytes: Optional[bytes] = None,
        product_id: Optional[str] = None
    ) -> str:
        """
        X·ª≠ l√Ω product v√† l∆∞u v√†o vector store
        
        Args:
            product_data: Dict ch·ª©a th√¥ng tin product
                - product_name: T√™n s·∫£n ph·∫©m
                - description: M√¥ t·∫£
                - category_id: ID category
                - category_name: T√™n category
                - price: Gi√°
                - etc.
            image_bytes: ·∫¢nh s·∫£n ph·∫©m (t√πy ch·ªçn)
            product_id: ID s·∫£n ph·∫©m (t√πy ch·ªçn, s·∫Ω l·∫•y t·ª´ product_data n·∫øu c√≥)
            
        Returns:
            product_id c·ªßa s·∫£n ph·∫©m ƒë√£ x·ª≠ l√Ω
        """
        # L·∫•y product_id
        if not product_id:
            product_id = product_data.get('product_id') or f"PROD-{str(uuid.uuid4())[:8]}"
        
        category_id = product_data.get('category_id', '')
        category_name = product_data.get('category_name', '')
        product_name = product_data.get('product_name', '')
        
        try:
            logger.info(f"üõçÔ∏è  B·∫Øt ƒë·∫ßu x·ª≠ l√Ω product: {product_name} (ID: {product_id}, Category: {category_id})")
            
            # B∆∞·ªõc 1: T·∫°o embeddings cho product
            logger.info(f"üî¢ ƒêang t·∫°o embeddings cho product...")
            embeddings = await self.product_embedding_service.create_product_embeddings(
                product_data,
                image_bytes
            )
            
            # S·ª≠ d·ª•ng combined embedding (text CLIP + image) ƒë·ªÉ h·ªó tr·ª£ c·∫£ text v√† image search
            # Strategy: T·∫°o text embedding b·∫±ng CLIP text encoder (512 dim) + image embedding (512 dim)
            # Weighted average: 60% text, 40% image (c√≥ th·ªÉ ƒëi·ªÅu ch·ªânh)
            primary_embedding = None
            
            # T·∫°o text embedding b·∫±ng CLIP text encoder (512 dim) t·ª´ product name + description
            from app.api.deps import get_image_embedding_service
            image_embedding_service = get_image_embedding_service()
            
            product_text = f"{product_name} {product_data.get('description', '')}".strip()
            text_clip_embedding = None
            if product_text:
                text_clip_embedding = image_embedding_service.create_text_embedding(product_text)
            
            image_emb = embeddings.get('image_embedding')
            
            # Combine: 70% text CLIP, 30% image (n·∫øu c√≥ c·∫£ 2)
            # TƒÉng weight c·ªßa text ƒë·ªÉ text search t·ªët h∆°n
            if text_clip_embedding is not None and image_emb is not None:
                # Normalize c·∫£ 2
                text_norm = text_clip_embedding / (np.linalg.norm(text_clip_embedding) + 1e-8)
                img_norm = image_emb / (np.linalg.norm(image_emb) + 1e-8)
                # Weighted average: 70% text, 30% image (tƒÉng weight text ƒë·ªÉ text search t·ªët h∆°n)
                primary_embedding = 0.7 * text_norm + 0.3 * img_norm
                logger.info(f"‚úÖ S·ª≠ d·ª•ng combined embedding (70% text CLIP + 30% image, dimension: {len(primary_embedding)})")
            elif text_clip_embedding is not None:
                # Ch·ªâ c√≥ text, d√πng text CLIP embedding
                primary_embedding = text_clip_embedding
                logger.info(f"‚úÖ S·ª≠ d·ª•ng text CLIP embedding (dimension: {len(primary_embedding)})")
            elif image_emb is not None:
                # Ch·ªâ c√≥ image, d√πng image embedding
                primary_embedding = image_emb
                logger.info(f"‚úÖ S·ª≠ d·ª•ng image embedding (dimension: {len(primary_embedding)})")
            else:
                logger.warning("‚ö†Ô∏è  Product kh√¥ng c√≥ text v√† image, kh√¥ng th·ªÉ t·∫°o embedding")
            
            if primary_embedding is None:
                raise ValueError("Kh√¥ng th·ªÉ t·∫°o embedding cho product")
            
            # B∆∞·ªõc 2: T·∫°o DocumentChunk t·ª´ product
            chunk = DocumentChunk(
                chunk_id=f"{product_id}-chunk-0",
                file_id=product_id,
                file_name=product_name or f"Product_{product_id}",
                text=f"[Product: {product_name}] {product_data.get('description', '')}",
                chunk_index=0,
                start_index=0,
                end_index=0
            )
            
            # B∆∞·ªõc 3: L∆∞u v√†o vector store v·ªõi metadata ƒë·∫ßy ƒë·ªß
            logger.info(f"üíæ ƒêang l∆∞u product v√†o vector store...")
            upload_date = datetime.now().isoformat()
            
            # L·∫•y image filename t·ª´ product_data n·∫øu c√≥ (t·ª´ database khi embed)
            image_filename = product_data.get('image_filename') or product_data.get('anh')
            
            # Metadata cho product
            # Convert price to float (ChromaDB doesn't accept Decimal)
            price_value = product_data.get('price', '')
            if price_value:
                try:
                    price_float = float(price_value)
                except (ValueError, TypeError):
                    price_float = 0.0
            else:
                price_float = 0.0
            
            extra_metadata = [{
                "product_id": product_id,
                "product_name": product_name,
                "category_id": category_id,
                "category_name": category_name,
                "content_type": "product",
                "price": price_float,  # Convert to float for ChromaDB
                "description": product_data.get('description', '')[:200] if product_data.get('description') else '',  # Limit length
                "image_filename": image_filename if image_filename else '',  # L∆∞u image filename ƒë·ªÉ d√πng sau
            }]
            
            await self.vector_store.save_chunks(
                [chunk],
                [primary_embedding],
                file_type="product",
                upload_date=upload_date,
                extra_metadata=extra_metadata
            )
            
            logger.info(f"‚úÖ ƒê√£ x·ª≠ l√Ω v√† l∆∞u th√†nh c√¥ng product {product_name} (Category: {category_id})")
            
            return product_id
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói khi x·ª≠ l√Ω product {product_name}: {str(e)}", exc_info=True)
            raise
    
    async def process_and_store_batch(
        self,
        products: List[Dict],
        images: Optional[List[bytes]] = None
    ) -> List[str]:
        """
        X·ª≠ l√Ω nhi·ªÅu products c√πng l√∫c (batch)
        
        Args:
            products: Danh s√°ch product data
            images: Danh s√°ch ·∫£nh t∆∞∆°ng ·ª©ng (t√πy ch·ªçn)
            
        Returns:
            Danh s√°ch product_id ƒë√£ x·ª≠ l√Ω
        """
        if not products:
            return []
        
        try:
            logger.info(f"üõçÔ∏è  B·∫Øt ƒë·∫ßu x·ª≠ l√Ω batch {len(products)} products...")
            
            product_ids = []
            chunks = []
            embeddings_list = []
            upload_date = datetime.now().isoformat()
            
            for i, product_data in enumerate(products):
                product_id = product_data.get('product_id') or f"PROD-{str(uuid.uuid4())[:8]}"
                image_bytes = images[i] if images and i < len(images) else None
                
                # T·∫°o embeddings
                embeddings = await self.product_embedding_service.create_product_embeddings(
                    product_data,
                    image_bytes
                )
                
                # S·ª≠ d·ª•ng image embedding
                primary_embedding = embeddings.get('image_embedding')
                if primary_embedding is None:
                    primary_embedding = embeddings.get('combined_embedding')
                
                if primary_embedding is None:
                    logger.warning(f"Skipping product {product_id}: kh√¥ng c√≥ embedding")
                    continue
                
                # T·∫°o chunk
                product_name = product_data.get('product_name', '')
                chunk = DocumentChunk(
                    chunk_id=f"{product_id}-chunk-0",
                    file_id=product_id,
                    file_name=product_name or f"Product_{product_id}",
                    text=f"[Product: {product_name}] {product_data.get('description', '')}",
                    chunk_index=0,
                    start_index=0,
                    end_index=0
                )
                
                chunks.append(chunk)
                embeddings_list.append(primary_embedding)
                product_ids.append(product_id)
            
            # L∆∞u t·∫•t c·∫£ c√πng l√∫c
            if chunks:
                # T·∫°o extra metadata cho t·ª´ng product
                extra_metadata_list = []
                for i, product_data in enumerate(products):
                    if i < len(product_ids):
                        extra_metadata_list.append({
                            "product_id": product_ids[i],
                            "product_name": product_data.get('product_name', ''),
                            "category_id": product_data.get('category_id', ''),
                            "category_name": product_data.get('category_name', ''),
                            "content_type": "product",
                            "price": str(product_data.get('price', '')),
                            "description": product_data.get('description', '')[:200] if product_data.get('description') else '',
                        })
                
                await self.vector_store.save_chunks(
                    chunks,
                    embeddings_list,
                    file_type="product",
                    upload_date=upload_date,
                    extra_metadata=extra_metadata_list
                )
            
            logger.info(f"‚úÖ ƒê√£ x·ª≠ l√Ω v√† l∆∞u th√†nh c√¥ng {len(product_ids)} products")
            
            return product_ids
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói khi x·ª≠ l√Ω batch products: {str(e)}", exc_info=True)
            raise

