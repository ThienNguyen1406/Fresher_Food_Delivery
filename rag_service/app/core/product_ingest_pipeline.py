"""
Product Ingest Pipeline - X·ª≠ l√Ω product v√† l∆∞u v√†o vector store theo category
Pipeline: Product (Text + Image) ‚Üí Embeddings ‚Üí Vector Database (theo category)
"""
import logging
import uuid
from datetime import datetime
from typing import Optional, List, Dict
import numpy as np

from app.services.product import ProductEmbeddingService
from app.infrastructure.vector_store.image_vector_store import ImageVectorStore
from app.domain.document import DocumentChunk

logger = logging.getLogger(__name__)


class ProductIngestPipeline:
    """
    Pipeline x·ª≠ l√Ω product v√† l∆∞u v√†o vector store theo category
    """
    
    def __init__(
        self,
        product_embedding_service: ProductEmbeddingService,
        vector_store: ImageVectorStore
    ):
        """
        Kh·ªüi t·∫°o Product Ingest Pipeline
        
        Args:
            product_embedding_service: Service t·∫°o embeddings cho product
            vector_store: Vector store ƒë·ªÉ l∆∞u embeddings
        """
        self.product_embedding_service = product_embedding_service
        self.vector_store = vector_store
    
    async def process_and_store(
        self,
        product_data: Dict,
        image_bytes: Optional[bytes] = None,
        product_id: Optional[str] = None
    ) -> str:
        """
        X·ª≠ l√Ω product v√† l∆∞u v√†o vector store
        
        Args:
            product_data: Dict ch·ª©a th√¥ng tin product
                - product_name: T√™n s·∫£n ph·∫©m
                - description: M√¥ t·∫£
                - category_id: ID category
                - category_name: T√™n category
                - price: Gi√°
                - etc.
            image_bytes: ·∫¢nh s·∫£n ph·∫©m (t√πy ch·ªçn)
            product_id: ID s·∫£n ph·∫©m (t√πy ch·ªçn, s·∫Ω l·∫•y t·ª´ product_data n·∫øu c√≥)
            
        Returns:
            product_id c·ªßa s·∫£n ph·∫©m ƒë√£ x·ª≠ l√Ω
        """
        # L·∫•y product_id
        if not product_id:
            product_id = product_data.get('product_id') or f"PROD-{str(uuid.uuid4())[:8]}"
        
        category_id = product_data.get('category_id', '')
        category_name = product_data.get('category_name', '')
        product_name = product_data.get('product_name', '')
        
        try:
            logger.info(f"üõçÔ∏è  B·∫Øt ƒë·∫ßu x·ª≠ l√Ω product: {product_name} (ID: {product_id}, Category: {category_id})")
            
            # T·∫°o embeddings cho product
            logger.info(f"üî¢ ƒêang t·∫°o embeddings cho product...")
            embeddings = await self.product_embedding_service.create_product_embeddings(
                product_data,
                image_bytes
            )
            
            # S·ª≠ d·ª•ng combined embedding (text CLIP + image) ƒë·ªÉ h·ªó tr·ª£ c·∫£ text v√† image search
            primary_embedding = None
            
            # T·∫°o text embedding b·∫±ng CLIP text encoder (512 dim) t·ª´ product name + description
            # QUAN TR·ªåNG: Enrich text v·ªõi semantic keywords ƒë·ªÉ embedding ch√≠nh x√°c h∆°n
            from app.api.deps import get_image_embedding_service
            image_embedding_service = get_image_embedding_service()
            
            # Enrich product text v·ªõi semantic information
            product_text = self._enrich_product_text(product_data, product_name)
            text_clip_embedding = None
            if product_text:
                text_clip_embedding = image_embedding_service.create_text_embedding(product_text)
            
            image_emb = embeddings.get('image_embedding')
            
            # TƒÉng weight c·ªßa text ƒë·ªÉ text search t·ªët h∆°n
            if text_clip_embedding is not None and image_emb is not None:
                # Normalize c·∫£ 2
                text_norm = text_clip_embedding / (np.linalg.norm(text_clip_embedding) + 1e-8)
                img_norm = image_emb / (np.linalg.norm(image_emb) + 1e-8)
                # Weighted average: 70% text, 30% image (tƒÉng weight text ƒë·ªÉ text search t·ªët h∆°n)
                primary_embedding = 0.7 * text_norm + 0.3 * img_norm
                logger.info(f"‚úÖ S·ª≠ d·ª•ng combined embedding (70% text CLIP + 30% image, dimension: {len(primary_embedding)})")
            elif text_clip_embedding is not None:
                # Ch·ªâ c√≥ text, d√πng text CLIP embedding
                primary_embedding = text_clip_embedding
                logger.info(f"‚úÖ S·ª≠ d·ª•ng text CLIP embedding (dimension: {len(primary_embedding)})")
            elif image_emb is not None:
                # Ch·ªâ c√≥ image, d√πng image embedding
                primary_embedding = image_emb
                logger.info(f"‚úÖ S·ª≠ d·ª•ng image embedding (dimension: {len(primary_embedding)})")
            else:
                logger.warning("‚ö†Ô∏è  Product kh√¥ng c√≥ text v√† image, kh√¥ng th·ªÉ t·∫°o embedding")
            
            if primary_embedding is None:
                raise ValueError("Kh√¥ng th·ªÉ t·∫°o embedding cho product")
            
            # T·∫°o DocumentChunk t·ª´ product
            chunk = DocumentChunk(
                chunk_id=f"{product_id}-chunk-0",
                file_id=product_id,
                file_name=product_name or f"Product_{product_id}",
                text=f"[Product: {product_name}] {product_data.get('description', '')}",
                chunk_index=0,
                start_index=0,
                end_index=0
            )
            
            # L∆∞u v√†o vector store v·ªõi metadata ƒë·∫ßy ƒë·ªß
            logger.info(f"üíæ ƒêang l∆∞u product v√†o vector store...")
            upload_date = datetime.now().isoformat()
            
            # L·∫•y image filename t·ª´ product_data n·∫øu c√≥ (t·ª´ database khi embed)
            image_filename = product_data.get('image_filename') or product_data.get('anh')
            
            # Metadata cho product
            # Convert price to float (ChromaDB doesn't accept Decimal)
            price_value = product_data.get('price', '')
            if price_value:
                try:
                    price_float = float(price_value)
                except (ValueError, TypeError):
                    price_float = 0.0
            else:
                price_float = 0.0
            
            extra_metadata = [{
                "product_id": product_id,
                "product_name": product_name,
                "category_id": category_id,
                "category_name": category_name,
                "content_type": "product",
                "price": price_float,  # Convert to float for ChromaDB
                "description": product_data.get('description', '')[:200] if product_data.get('description') else '',  # Limit length
                "image_filename": image_filename if image_filename else '',  # L∆∞u image filename ƒë·ªÉ d√πng sau
            }]
            
            await self.vector_store.save_chunks(
                [chunk],
                [primary_embedding],
                file_type="product",
                upload_date=upload_date,
                extra_metadata=extra_metadata
            )
            
            logger.info(f"‚úÖ ƒê√£ x·ª≠ l√Ω v√† l∆∞u th√†nh c√¥ng product {product_name} (Category: {category_id})")
            
            return product_id
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói khi x·ª≠ l√Ω product {product_name}: {str(e)}", exc_info=True)
            raise
    
    async def process_and_store_batch(
        self,
        products: List[Dict],
        images: Optional[List[bytes]] = None
    ) -> List[str]:
        """
        X·ª≠ l√Ω nhi·ªÅu products c√πng l√∫c (batch)
        
        Args:
            products: Danh s√°ch product data
            images: Danh s√°ch ·∫£nh t∆∞∆°ng ·ª©ng (t√πy ch·ªçn)
            
        Returns:
            Danh s√°ch product_id ƒë√£ x·ª≠ l√Ω
        """
        if not products:
            return []
        
        try:
            logger.info(f"üõçÔ∏è  B·∫Øt ƒë·∫ßu x·ª≠ l√Ω batch {len(products)} products...")
            
            product_ids = []
            chunks = []
            embeddings_list = []
            upload_date = datetime.now().isoformat()
            
            for i, product_data in enumerate(products):
                product_id = product_data.get('product_id') or f"PROD-{str(uuid.uuid4())[:8]}"
                image_bytes = images[i] if images and i < len(images) else None
                
                # T·∫°o embeddings
                embeddings = await self.product_embedding_service.create_product_embeddings(
                    product_data,
                    image_bytes
                )
                
                # S·ª≠ d·ª•ng image embedding
                primary_embedding = embeddings.get('image_embedding')
                if primary_embedding is None:
                    primary_embedding = embeddings.get('combined_embedding')
                
                if primary_embedding is None:
                    logger.warning(f"Skipping product {product_id}: kh√¥ng c√≥ embedding")
                    continue
                
                # T·∫°o chunk
                product_name = product_data.get('product_name', '')
                chunk = DocumentChunk(
                    chunk_id=f"{product_id}-chunk-0",
                    file_id=product_id,
                    file_name=product_name or f"Product_{product_id}",
                    text=f"[Product: {product_name}] {product_data.get('description', '')}",
                    chunk_index=0,
                    start_index=0,
                    end_index=0
                )
                
                chunks.append(chunk)
                embeddings_list.append(primary_embedding)
                product_ids.append(product_id)
            
            # L∆∞u t·∫•t c·∫£ c√πng l√∫c
            if chunks:
                # T·∫°o extra metadata cho t·ª´ng product
                extra_metadata_list = []
                for i, product_data in enumerate(products):
                    if i < len(product_ids):
                        extra_metadata_list.append({
                            "product_id": product_ids[i],
                            "product_name": product_data.get('product_name', ''),
                            "category_id": product_data.get('category_id', ''),
                            "category_name": product_data.get('category_name', ''),
                            "content_type": "product",
                            "price": str(product_data.get('price', '')),
                            "description": product_data.get('description', '')[:200] if product_data.get('description') else '',
                        })
                
                await self.vector_store.save_chunks(
                    chunks,
                    embeddings_list,
                    file_type="product",
                    upload_date=upload_date,
                    extra_metadata=extra_metadata_list
                )
            
            logger.info(f"‚úÖ ƒê√£ x·ª≠ l√Ω v√† l∆∞u th√†nh c√¥ng {len(product_ids)} products")
            
            return product_ids
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói khi x·ª≠ l√Ω batch products: {str(e)}", exc_info=True)
            raise
    
    def _enrich_product_text(self, product_data: Dict, product_name: str) -> str:
        """
        Enrich product text v·ªõi semantic keywords ƒë·ªÉ embedding ch√≠nh x√°c
        Args:
            product_data: Dict ch·ª©a th√¥ng tin product
            product_name: T√™n s·∫£n ph·∫©m
            
        Returns:
            Text ƒë√£ ƒë∆∞·ª£c enrich v·ªõi semantic keywords
        """
        text_parts = []
        
        # T√™n s·∫£n ph·∫©m g·ªëc
        if product_name:
            text_parts.append(product_name)
        
        #  M√¥ t·∫£ (n·∫øu c√≥)
        description = product_data.get('description', '')
        if description:
            text_parts.append(description)
        
        # Category name (quan tr·ªçng ƒë·ªÉ ph√¢n bi·ªát category)
        category_name = product_data.get('category_name', '')
        if category_name:
            text_parts.append(category_name)
        
        #  Th√™m semantic keywords d·ª±a tr√™n category v√† product name
        # ƒêi·ªÅu n√†y gi√∫p ph√¢n bi·ªát r√µ c√°c lo·∫°i s·∫£n ph·∫©m kh√°c nhau
        semantic_keywords = self._extract_semantic_keywords(product_name, category_name, description)
        if semantic_keywords:
            text_parts.extend(semantic_keywords)
        
        #  Origin v√† Unit (n·∫øu c√≥)
        origin = product_data.get('origin', '')
        if origin:
            text_parts.append(f"from {origin}")
        
        unit = product_data.get('unit', '')
        if unit:
            text_parts.append(f"unit {unit}")
        
        return " ".join(text_parts)
    
    def _extract_semantic_keywords(self, product_name: str, category_name: str, description: str) -> List[str]:
        """
        Extract semantic keywords t·ª´ product name v√† category
        ƒê·ªÉ gi√∫p embedding ph√¢n bi·ªát r√µ c√°c lo·∫°i s·∫£n ph·∫©m
        """
        keywords = []
        product_lower = product_name.lower()
        category_lower = category_name.lower() if category_name else ""
        desc_lower = description.lower() if description else ""
        
        # Keywords d·ª±a tr√™n category
        if "ƒë·ªì u·ªëng" in category_lower or "drink" in category_lower or "beverage" in category_lower:
            keywords.extend(["drink", "beverage", "liquid"])
            if "s·ªØa" in product_lower or "milk" in product_lower:
                keywords.extend(["milk", "dairy", "carton", "bottle"])
            if "n∆∞·ªõc" in product_lower or "water" in product_lower:
                keywords.extend(["water", "mineral", "bottled"])
            if "milo" in product_lower or "ovaltine" in product_lower or "cacao" in product_lower:
                keywords.extend(["chocolate", "malt", "powder", "instant"])
        
        if "th·ªãt" in category_lower or "meat" in category_lower:
            keywords.extend(["meat", "protein", "fresh", "raw", "food"])
            if "b√≤" in product_lower or "beef" in product_lower:
                keywords.extend(["beef", "cow", "red meat"])
            if "heo" in product_lower or "pork" in product_lower:
                keywords.extend(["pork", "pig"])
            if "g√†" in product_lower or "chicken" in product_lower:
                keywords.extend(["chicken", "poultry"])
        
        if "rau" in category_lower or "vegetable" in category_lower:
            keywords.extend(["vegetable", "fresh", "green", "produce"])
        
        if "tr√°i c√¢y" in category_lower or "fruit" in category_lower:
            keywords.extend(["fruit", "fresh", "sweet", "produce"])
        
        if "c√°" in category_lower or "fish" in category_lower:
            keywords.extend(["fish", "seafood", "protein", "fresh"])
        
        # Keywords d·ª±a tr√™n product name
        if "milo" in product_lower:
            keywords.extend(["nestl√©", "chocolate", "malt", "milk", "drink", "carton"])
        if "n∆∞·ªõc su·ªëi" in product_lower or "mineral water" in product_lower:
            keywords.extend(["water", "mineral", "bottled", "pure"])
        if "th·ªãt b√≤" in product_lower or "beef" in product_lower:
            keywords.extend(["beef", "cow", "red meat", "protein"])
        if "s·ªØa" in product_lower and "milo" not in product_lower:
            keywords.extend(["milk", "dairy", "white", "liquid"])
        
        # Keywords t·ª´ description n·∫øu c√≥
        if "chocolate" in desc_lower:
            keywords.append("chocolate")
        if "malt" in desc_lower:
            keywords.append("malt")
        if "carton" in desc_lower or "h·ªôp" in desc_lower:
            keywords.append("carton")
        if "bottle" in desc_lower or "chai" in desc_lower:
            keywords.append("bottle")
        
        # Lo·∫°i b·ªè duplicates v√† tr·∫£ v·ªÅ
        return list(set(keywords))

