"""
Image Embedding Service - Service t·∫°o embedding vectors t·ª´ ·∫£nh
Chuy·ªÉn ƒë·ªïi ·∫£nh th√†nh c√°c vector s·ªë ƒë·ªÉ c√≥ th·ªÉ so s√°nh v√† t√¨m ki·∫øm
"""
import os
import logging
from typing import Optional, List
import numpy as np
from PIL import Image
import io
import base64

from app.core.settings import Settings

logger = logging.getLogger(__name__)


class ImageEmbeddingService:
    """
    Service t·∫°o embedding vectors t·ª´ ·∫£nh
    
    Image Embedding l√† c√°ch chuy·ªÉn ƒë·ªïi ·∫£nh th√†nh c√°c vector s·ªë ƒë·ªÉ:
    - So s√°nh ƒë·ªô t∆∞∆°ng ƒë·ªìng gi·ªØa c√°c ·∫£nh
    - T√¨m ki·∫øm ·∫£nh t∆∞∆°ng t·ª± (image similarity search)
    - L∆∞u tr·ªØ v√† t√¨m ki·∫øm trong vector database
    """
    
    def __init__(self):
        """Kh·ªüi t·∫°o Image Embedding Service"""
        self.embedding_model = None
        self.use_openai = Settings.USE_OPENAI_EMBEDDINGS
        self.openai_api_key = Settings.OPENAI_API_KEY
        
        # L∆∞u √Ω: OpenAI kh√¥ng c√≥ direct image embedding API nh∆∞ text embedding
        # Hi·ªán t·∫°i ch·ªâ h·ªó tr·ª£ CLIP model cho image embeddings
        # Lu√¥n kh·ªüi t·∫°o CLIP (ngay c·∫£ khi c√≥ OpenAI key)
        logger.info("üîÑ ƒêang kh·ªüi t·∫°o CLIP model cho image embeddings")
        self._init_clip()
        
        # Kh·ªüi t·∫°o OpenAI client n·∫øu c√≥ key (ƒë·ªÉ d√πng cho c√°c t√≠nh nƒÉng kh√°c trong t∆∞∆°ng lai)
        if self.use_openai and self.openai_api_key:
            logger.info("‚úÖ OpenAI API key ƒë√£ ƒë∆∞·ª£c c·∫•u h√¨nh (d√πng cho c√°c t√≠nh nƒÉng kh√°c)")
            self._init_openai()
        elif self.use_openai:
            logger.warning("‚ö†Ô∏è  OpenAI embeddings ƒë∆∞·ª£c b·∫≠t nh∆∞ng ch∆∞a c√≥ API Key!")
            logger.warning("   ƒê·ªÉ c·∫•u h√¨nh: Th√™m OPENAI_API_KEY v√†o file .env")
    
    def _init_openai(self):
        """Kh·ªüi t·∫°o OpenAI vision embeddings"""
        try:
            import openai
            from openai import OpenAI
            
            self.openai_client = OpenAI(
                api_key=self.openai_api_key,
                timeout=60.0,
                max_retries=2
            )
            # S·ª≠ d·ª•ng OpenAI vision model cho ·∫£nh
            self.embedding_model = "clip-vit-base-patch32"  # Ho·∫∑c c√≥ th·ªÉ d√πng OpenAI vision API
            logger.info("OpenAI image embedding client initialized")
        except ImportError:
            logger.warning("OpenAI library ch∆∞a ƒë∆∞·ª£c c√†i ƒë·∫∑t, chuy·ªÉn sang CLIP")
            self.use_openai = False
            self._init_clip()
    
    def _init_clip(self):
        """Kh·ªüi t·∫°o CLIP model (fallback khi kh√¥ng c√≥ OpenAI)"""
        try:
            import clip
            import torch
            
            # Load CLIP model
            device = "cuda" if torch.cuda.is_available() else "cpu"
            model_name = "ViT-B/32"  # CLIP ViT-B/32 model
            
            logger.info(f"ƒêang t·∫£i CLIP model: {model_name} (device: {device})")
            self.clip_model, self.clip_preprocess = clip.load(model_name, device=device)
            # CLIP model ƒë√£ c√≥ tokenizer built-in, kh√¥ng c·∫ßn load ri√™ng
            # Tokenizer ƒë∆∞·ª£c truy c·∫≠p qua clip.tokenize()
            self.clip_device = device
            self.embedding_model = model_name
            
            logger.info(f"‚úÖ ƒê√£ t·∫£i CLIP model: {model_name}")
        except ImportError:
            logger.error("CLIP library ch∆∞a ƒë∆∞·ª£c c√†i ƒë·∫∑t. Vui l√≤ng c√†i: pip install git+https://github.com/openai/CLIP.git")
            raise
        except Exception as e:
            logger.error(f"L·ªói khi t·∫£i CLIP model: {str(e)}")
            raise
    
    def _preprocess_image(self, image_bytes: bytes) -> Image.Image:
        """Ti·ªÅn x·ª≠ l√Ω ·∫£nh: resize, normalize, etc."""
        try:
            image = Image.open(io.BytesIO(image_bytes))
            # Convert to RGB n·∫øu c·∫ßn
            if image.mode != 'RGB':
                image = image.convert('RGB')
            return image
        except Exception as e:
            logger.error(f"L·ªói khi x·ª≠ l√Ω ·∫£nh: {str(e)}")
            raise
    
    async def create_embedding(self, image_bytes: bytes) -> Optional[np.ndarray]:
        """
        T·∫°o embedding vector t·ª´ ·∫£nh
        
        Args:
            image_bytes: ·∫¢nh d∆∞·ªõi d·∫°ng bytes
            
        Returns:
            Embedding vector (numpy array) ho·∫∑c None n·∫øu l·ªói
        """
        if not image_bytes:
            return None
        
        try:
            # Hi·ªán t·∫°i ch·ªâ d√πng CLIP (OpenAI kh√¥ng c√≥ direct image embedding API)
            # N·∫øu c√≥ OpenAI key, c√≥ th·ªÉ d√πng ƒë·ªÉ m√¥ t·∫£ ·∫£nh r·ªìi embed text, nh∆∞ng CLIP t·ªët h∆°n cho similarity
            return self._create_clip_embedding(image_bytes)
        except Exception as e:
            logger.error(f"Error creating image embedding: {str(e)}")
            return None
    
    async def _create_openai_embedding(self, image_bytes: bytes) -> np.ndarray:
        """
        T·∫°o embedding s·ª≠ d·ª•ng OpenAI vision API
        
        L∆∞u √Ω: OpenAI kh√¥ng c√≥ direct image embedding API nh∆∞ text embedding.
        C√≥ th·ªÉ d√πng OpenAI Vision ƒë·ªÉ m√¥ t·∫£ ·∫£nh, r·ªìi embed text description,
        nh∆∞ng CLIP t·ªët h∆°n cho image similarity search.
        """
        # Fallback v·ªÅ CLIP v√¨ OpenAI kh√¥ng c√≥ direct image embedding
        logger.warning("OpenAI kh√¥ng c√≥ direct image embedding API, d√πng CLIP")
        return self._create_clip_embedding(image_bytes)
    
    def create_text_embedding(self, text: str) -> Optional[np.ndarray]:
        """
        T·∫°o text embedding s·ª≠ d·ª•ng CLIP text encoder (512 dim)
        T∆∞∆°ng th√≠ch v·ªõi image embedding ƒë·ªÉ search products
        
        Args:
            text: Text c·∫ßn embed
            
        Returns:
            Text embedding vector (512 dimensions - CLIP)
        """
        if not text or not text.strip():
            return None
        
        try:
            import torch
            
            if not self.clip_model:
                logger.error("CLIP model ch∆∞a ƒë∆∞·ª£c kh·ªüi t·∫°o")
                return None
            
            # Tokenize text using CLIP's built-in tokenizer
            import clip
            text_tokens = clip.tokenize([text], truncate=True).to(self.clip_device)
            
            # Generate embedding
            with torch.no_grad():
                text_features = self.clip_model.encode_text(text_tokens)
                # Normalize features
                text_features = text_features / text_features.norm(dim=-1, keepdim=True)
                embedding = text_features.cpu().numpy()[0]
            
            return embedding.astype(np.float32)
        except Exception as e:
            logger.error(f"Error creating CLIP text embedding: {str(e)}")
            return None
    
    def create_query_embedding(
        self,
        image_bytes: Optional[bytes] = None,
        caption: Optional[str] = None
    ) -> Optional[np.ndarray]:
        """
        üî• T·ªêI ∆ØU: T·∫°o query embedding t·ª´ image + caption (n·∫øu c√≥)
        Service layer ch·ªãu tr√°ch nhi·ªám normalize + combine
        API layer KH√îNG ƒë∆∞·ª£c normalize
        
        Args:
            image_bytes: ·∫¢nh query (t√πy ch·ªçn)
            caption: Text caption t·ª´ Vision (t√πy ch·ªçn)
            
        Returns:
            Query embedding ƒë√£ normalize + combine (60% image + 40% caption n·∫øu c√≥ c·∫£ 2)
        """
        image_emb = None
        text_emb = None
        
        # T·∫°o image embedding
        if image_bytes:
            image_emb = self._create_clip_embedding(image_bytes)
        
        # T·∫°o text embedding t·ª´ caption
        if caption:
            text_emb = self.create_text_embedding(caption)
        
        # Combine: 60% image + 40% text (n·∫øu c√≥ c·∫£ 2)
        if image_emb is not None and text_emb is not None:
            # Normalize c·∫£ 2
            img_norm = image_emb / (np.linalg.norm(image_emb) + 1e-8)
            txt_norm = text_emb / (np.linalg.norm(text_emb) + 1e-8)
            # Weighted average: 60% image, 40% text
            combined = 0.6 * img_norm + 0.4 * txt_norm
            # Normalize l·∫°i sau khi combine
            combined = combined / (np.linalg.norm(combined) + 1e-8)
            return combined.astype(np.float32)
        elif image_emb is not None:
            # Ch·ªâ c√≥ image (ƒë√£ normalize trong CLIP)
            return image_emb
        elif text_emb is not None:
            # Ch·ªâ c√≥ text (ƒë√£ normalize trong CLIP)
            return text_emb
        
        return None
    
    def _create_clip_embedding(self, image_bytes: bytes) -> np.ndarray:
        """T·∫°o embedding s·ª≠ d·ª•ng CLIP model"""
        try:
            import torch
            
            # Preprocess image
            image = self._preprocess_image(image_bytes)
            image_tensor = self.clip_preprocess(image).unsqueeze(0).to(self.clip_device)
            
            # Generate embedding
            with torch.no_grad():
                image_features = self.clip_model.encode_image(image_tensor)
                # Normalize features
                image_features = image_features / image_features.norm(dim=-1, keepdim=True)
                embedding = image_features.cpu().numpy()[0]
            
            return embedding.astype(np.float32)
        except Exception as e:
            logger.error(f"L·ªói khi t·∫°o CLIP embedding: {str(e)}")
            raise
    
    async def create_embeddings(self, images: List[bytes]) -> List[Optional[np.ndarray]]:
        """
        T·∫°o embeddings cho nhi·ªÅu ·∫£nh (batch)
        
        Args:
            images: Danh s√°ch ·∫£nh d∆∞·ªõi d·∫°ng bytes
            
        Returns:
            Danh s√°ch embedding vectors
        """
        if not images:
            return []
        
        try:
            # Hi·ªán t·∫°i ch·ªâ d√πng CLIP batch processing
            return self._create_clip_embeddings_batch(images)
        except Exception as e:
            logger.error(f"L·ªói khi t·∫°o image embeddings: {str(e)}", exc_info=True)
            return [None] * len(images)
    
    def _create_clip_embeddings_batch(self, images: List[bytes]) -> List[Optional[np.ndarray]]:
        """T·∫°o embeddings cho nhi·ªÅu ·∫£nh c√πng l√∫c b·∫±ng CLIP"""
        try:
            import torch
            
            # Preprocess t·∫•t c·∫£ ·∫£nh
            image_tensors = []
            for img_bytes in images:
                try:
                    image = self._preprocess_image(img_bytes)
                    image_tensor = self.clip_preprocess(image)
                    image_tensors.append(image_tensor)
                except Exception as e:
                    logger.error(f"L·ªói khi preprocess ·∫£nh: {str(e)}")
                    image_tensors.append(None)
            
            # Filter out None values
            valid_indices = [i for i, tensor in enumerate(image_tensors) if tensor is not None]
            valid_tensors = [image_tensors[i] for i in valid_indices]
            
            if not valid_tensors:
                return [None] * len(images)
            
            # Batch process
            batch_tensor = torch.stack(valid_tensors).to(self.clip_device)
            
            with torch.no_grad():
                image_features = self.clip_model.encode_image(batch_tensor)
                # Normalize features
                image_features = image_features / image_features.norm(dim=-1, keepdim=True)
                embeddings = image_features.cpu().numpy()
            
            # Map back to original list
            result = [None] * len(images)
            for idx, valid_idx in enumerate(valid_indices):
                result[valid_idx] = embeddings[idx].astype(np.float32)
            
            return result
        except Exception as e:
            logger.error(f"L·ªói khi t·∫°o CLIP embeddings batch: {str(e)}")
            return [None] * len(images)

