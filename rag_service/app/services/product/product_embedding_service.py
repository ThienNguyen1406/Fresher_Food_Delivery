"""
Product Embedding Service - Service t·∫°o embedding vectors t·ª´ product (text + image)
H·ªó tr·ª£: Image to Image search, Text to Image search
"""
import logging
from typing import Optional, List, Dict
import numpy as np

from app.services.image import ImageEmbeddingService
from app.services.embedding import EmbeddingService

logger = logging.getLogger(__name__)


class ProductEmbeddingService:
    """
    Service t·∫°o embedding vectors cho products
    
    H·ªó tr·ª£:
    - Image embeddings: T·ª´ ·∫£nh s·∫£n ph·∫©m (CLIP)
    - Text embeddings: T·ª´ t√™n, m√¥ t·∫£ s·∫£n ph·∫©m (OpenAI/Sentence Transformer)
    - Combined embeddings: K·∫øt h·ª£p text + image
    """
    
    def __init__(
        self,
        image_embedding_service: ImageEmbeddingService,
        text_embedding_service: EmbeddingService
    ):
        """
        Kh·ªüi t·∫°o Product Embedding Service
        
        Args:
            image_embedding_service: Service t·∫°o embedding t·ª´ ·∫£nh
            text_embedding_service: Service t·∫°o embedding t·ª´ text
        """
        self.image_embedding_service = image_embedding_service
        self.text_embedding_service = text_embedding_service
    
    async def create_image_embedding(self, image_bytes: bytes) -> Optional[np.ndarray]:
        """
        T·∫°o image embedding t·ª´ ·∫£nh s·∫£n ph·∫©m
        
        Args:
            image_bytes: ·∫¢nh s·∫£n ph·∫©m d∆∞·ªõi d·∫°ng bytes
            
        Returns:
            Image embedding vector (512 dimensions - CLIP)
        """
        return await self.image_embedding_service.create_embedding(image_bytes)
    
    async def create_text_embedding(self, text: str) -> Optional[np.ndarray]:
        """
        T·∫°o text embedding t·ª´ text s·∫£n ph·∫©m (t√™n, m√¥ t·∫£)
        
        Args:
            text: Text s·∫£n ph·∫©m (t√™n + m√¥ t·∫£)
            
        Returns:
            Text embedding vector (3072 dimensions - OpenAI ho·∫∑c 384 - Sentence Transformer)
        """
        if not text or not text.strip():
            return None
        return await self.text_embedding_service.create_embedding(text)
    
    async def create_combined_embedding(
        self,
        text: str,
        image_bytes: Optional[bytes] = None
    ) -> Optional[np.ndarray]:
        """
        T·∫°o combined embedding t·ª´ text + image
        
        Strategy: Normalize v√† concatenate ho·∫∑c weighted average
        
        Args:
            text: Text s·∫£n ph·∫©m
            image_bytes: ·∫¢nh s·∫£n ph·∫©m (t√πy ch·ªçn)
            
        Returns:
            Combined embedding vector
        """
        embeddings = []
        
        # Text embedding
        if text and text.strip():
            text_emb = await self.create_text_embedding(text)
            if text_emb is not None:
                # Normalize text embedding
                text_emb_norm = text_emb / (np.linalg.norm(text_emb) + 1e-8)
                embeddings.append(('text', text_emb_norm))
        
        # Image embedding
        if image_bytes:
            img_emb = await self.create_image_embedding(image_bytes)
            if img_emb is not None:
                # Normalize image embedding
                img_emb_norm = img_emb / (np.linalg.norm(img_emb) + 1e-8)
                embeddings.append(('image', img_emb_norm))
        
        if not embeddings:
            return None
        
        # Strategy: Weighted average (c√≥ th·ªÉ ƒëi·ªÅu ch·ªânh weights)
        # Text: 0.6, Image: 0.4 (c√≥ th·ªÉ ƒëi·ªÅu ch·ªânh)
        if len(embeddings) == 2:
            # C√≥ c·∫£ text v√† image
            text_emb = embeddings[0][1]
            img_emb = embeddings[1][1]
            
            # Resize ƒë·ªÉ c√πng dimension (l·∫•y min dimension)
            min_dim = min(len(text_emb), len(img_emb))
            text_emb_resized = text_emb[:min_dim]
            img_emb_resized = img_emb[:min_dim]
            
            # Weighted average
            combined = 0.6 * text_emb_resized + 0.4 * img_emb_resized
            return combined.astype(np.float32)
        else:
            # Ch·ªâ c√≥ m·ªôt lo·∫°i embedding
            return embeddings[0][1].astype(np.float32)
    
    def _normalize(self, v: np.ndarray) -> np.ndarray:
        """
        Normalize vector (helper function)
        
        Args:
            v: Vector c·∫ßn normalize
            
        Returns:
            Normalized vector
        """
        norm = np.linalg.norm(v)
        if norm < 1e-8:
            return v
        return v / norm
    
    async def create_product_embeddings(
        self,
        product_data: Dict,
        image_bytes: Optional[bytes] = None
    ) -> Dict[str, Optional[np.ndarray]]:
        """
        üî• T·ªêI ∆ØU: T·∫°o embeddings cho m·ªôt product - TR·∫¢ PRIMARY_EMBEDDING ƒê√É NORMALIZE + COMBINE
        
        Args:
            product_data: Dict ch·ª©a th√¥ng tin product
                - product_name: T√™n s·∫£n ph·∫©m
                - description: M√¥ t·∫£
                - category_name: T√™n category
                - origin: Xu·∫•t x·ª©
                - unit: ƒê∆°n v·ªã t√≠nh
            image_bytes: ·∫¢nh s·∫£n ph·∫©m (t√πy ch·ªçn)
            
        Returns:
            Dict ch·ª©a c√°c embeddings:
                - image_embedding: Image embedding (512 dim) - raw, ch∆∞a normalize
                - text_embedding: Text embedding (512 dim CLIP) - raw, ch∆∞a normalize
                - primary_embedding: PRIMARY embedding ƒë√£ normalize + combine (70% text CLIP + 30% image)
        """
        # T·∫°o text t·ª´ product data - ENRICH v·ªõi th√¥ng tin chi ti·∫øt
        product_name = product_data.get('product_name', '')
        description = product_data.get('description', '')
        category_name = product_data.get('category_name', '')
        origin = product_data.get('origin', '')
        unit = product_data.get('unit', '')
        
        # ENRICH: T·∫°o text m√¥ t·∫£ chi ti·∫øt h∆°n
        text_parts = []
        if product_name:
            text_parts.append(product_name)
        if description:
            text_parts.append(description)
        if origin:
            text_parts.append(f"Origin: {origin}")
        if unit:
            text_parts.append(f"Unit: {unit}")
        if category_name:
            text_parts.append(f"Category: {category_name}")
        
        text = " ".join(text_parts)
        
        # üî• T·ªêI ∆ØU: T·∫°o embeddings song song (n·∫øu c√≥ c·∫£ text v√† image)
        results = {}
        
        # Image embedding (CLIP - 512 dim)
        image_emb = None
        if image_bytes:
            image_emb = await self.create_image_embedding(image_bytes)
            results['image_embedding'] = image_emb
        
        # Text embedding (CLIP text encoder - 512 dim) - QUAN TR·ªåNG: D√πng CLIP text ƒë·ªÉ t∆∞∆°ng th√≠ch v·ªõi image
        text_clip_emb = None
        if text:
            # üî• D√πng CLIP text encoder (t·ª´ image_embedding_service) ƒë·ªÉ t∆∞∆°ng th√≠ch v·ªõi image embedding
            text_clip_emb = self.image_embedding_service.create_text_embedding(text)
            results['text_embedding'] = text_clip_emb
        
        # üî• T·ªêI ∆ØU: T·∫°o PRIMARY_EMBEDDING ƒë√£ normalize + combine (70% text CLIP + 30% image)
        # Strategy: 70% text CLIP (ƒë·ªÉ text search t·ªët) + 30% image (ƒë·ªÉ image search t·ªët)
        primary_embedding = None
        
        if text_clip_emb is not None and image_emb is not None:
            # C√≥ c·∫£ text v√† image ‚Üí combine v·ªõi weight
            text_norm = self._normalize(text_clip_emb)
            img_norm = self._normalize(image_emb)
            primary_embedding = 0.7 * text_norm + 0.3 * img_norm
            # Normalize l·∫°i sau khi combine
            primary_embedding = self._normalize(primary_embedding)
            logger.debug(f"‚úÖ Combined embedding (70% text CLIP + 30% image, dim: {len(primary_embedding)})")
        elif text_clip_emb is not None:
            # Ch·ªâ c√≥ text ‚Üí d√πng text CLIP (ƒë√£ normalize trong CLIP model)
            primary_embedding = self._normalize(text_clip_emb)
            logger.debug(f"‚úÖ Text CLIP embedding (dim: {len(primary_embedding)})")
        elif image_emb is not None:
            # Ch·ªâ c√≥ image ‚Üí d√πng image (ƒë√£ normalize trong CLIP model)
            primary_embedding = self._normalize(image_emb)
            logger.debug(f"‚úÖ Image embedding (dim: {len(primary_embedding)})")
        
        results['primary_embedding'] = primary_embedding
        
        return results
    
    async def create_embeddings_batch(
        self,
        products: List[Dict],
        images: Optional[List[bytes]] = None
    ) -> List[Dict[str, Optional[np.ndarray]]]:
        """
        üî• T·ªêI ∆ØU: T·∫°o embeddings cho nhi·ªÅu products c√πng l√∫c (BATCH TH·∫¨T)
        
        Args:
            products: Danh s√°ch product data
            images: Danh s√°ch ·∫£nh t∆∞∆°ng ·ª©ng (t√πy ch·ªçn)
            
        Returns:
            Danh s√°ch Dict embeddings cho t·ª´ng product:
                - image_embedding: Image embedding (512 dim)
                - text_embedding: Text embedding (512 dim CLIP)
                - primary_embedding: PRIMARY embedding ƒë√£ normalize + combine
        """
        if not products:
            return []
        
        # üî• B∆∞·ªõc 1: Chu·∫©n b·ªã texts v√† images cho batch
        texts = []
        image_list = []
        
        for i, product_data in enumerate(products):
            # T·∫°o text t·ª´ product data
            product_name = product_data.get('product_name', '')
            description = product_data.get('description', '')
            category_name = product_data.get('category_name', '')
            origin = product_data.get('origin', '')
            unit = product_data.get('unit', '')
            
            text_parts = []
            if product_name:
                text_parts.append(product_name)
            if description:
                text_parts.append(description)
            if origin:
                text_parts.append(f"Origin: {origin}")
            if unit:
                text_parts.append(f"Unit: {unit}")
            if category_name:
                text_parts.append(f"Category: {category_name}")
            
            text = " ".join(text_parts)
            texts.append(text if text else "")
            
            # L·∫•y image t∆∞∆°ng ·ª©ng
            if images and i < len(images) and images[i]:
                image_list.append(images[i])
            else:
                image_list.append(None)
        
        # üî• B∆∞·ªõc 2: Batch embed texts (CLIP text encoder)
        text_embeddings = []
        valid_texts = [(i, t) for i, t in enumerate(texts) if t]
        if valid_texts:
            # Batch process texts v·ªõi CLIP text encoder
            # CLIP text encoder c√≥ th·ªÉ batch, nh∆∞ng hi·ªán t·∫°i ch·ªâ c√≥ single text method
            # TODO: Optimize ƒë·ªÉ batch th·∫≠t n·∫øu CLIP h·ªó tr·ª£
            for idx, text in valid_texts:
                text_emb = self.image_embedding_service.create_text_embedding(text)
                text_embeddings.append((idx, text_emb))
        
        # üî• B∆∞·ªõc 3: Batch embed images (CLIP)
        image_embeddings = []
        valid_images = [(i, img) for i, img in enumerate(image_list) if img]
        if valid_images:
            image_bytes_list = [img for _, img in valid_images]
            # Batch process images v·ªõi CLIP
            batch_image_embs = await self.image_embedding_service.create_embeddings(image_bytes_list)
            for idx, (orig_idx, _) in enumerate(valid_images):
                if idx < len(batch_image_embs) and batch_image_embs[idx] is not None:
                    image_embeddings.append((orig_idx, batch_image_embs[idx]))
        
        # üî• B∆∞·ªõc 4: Combine embeddings cho t·ª´ng product
        results = []
        text_emb_dict = {idx: emb for idx, emb in text_embeddings}
        image_emb_dict = {idx: emb for idx, emb in image_embeddings}
        
        for i in range(len(products)):
            text_clip_emb = text_emb_dict.get(i)
            image_emb = image_emb_dict.get(i)
            
            result = {
                'text_embedding': text_clip_emb,
                'image_embedding': image_emb,
                'primary_embedding': None
            }
            
            # T·∫°o primary_embedding (70% text CLIP + 30% image)
            if text_clip_emb is not None and image_emb is not None:
                text_norm = self._normalize(text_clip_emb)
                img_norm = self._normalize(image_emb)
                primary_emb = 0.7 * text_norm + 0.3 * img_norm
                result['primary_embedding'] = self._normalize(primary_emb)
            elif text_clip_emb is not None:
                result['primary_embedding'] = self._normalize(text_clip_emb)
            elif image_emb is not None:
                result['primary_embedding'] = self._normalize(image_emb)
            
            results.append(result)
        
        logger.info(f"‚úÖ ƒê√£ t·∫°o batch embeddings cho {len(products)} products")
        return results

