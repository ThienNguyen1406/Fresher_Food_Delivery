"""
Entity Resolver Agent - X√°c ƒë·ªãnh entity (s·∫£n ph·∫©m) t·ª´ query
"""
from typing import Dict, Any, Optional, List
import logging
import re
from functools import lru_cache
from app.agents.base_agent import BaseAgent
from app.core.settings import Settings

logger = logging.getLogger(__name__)


class EntityResolverAgent(BaseAgent):
    """
    Entity Resolver Agents
    """
    
    def __init__(self):
        super().__init__("EntityResolverAgent")
        
        # üî• Synonym map cho c√°c s·∫£n ph·∫©m ph·ªï bi·∫øn
        self.synonym_map = {
            "c√° h·ªìi": ["c√° h·ªìi", "salmon", "c√° h·ªìi na uy", "c√° h·ªìi t∆∞∆°i", "c√° h·ªìi nauy"],
            "th·ªãt b√≤": ["th·ªãt b√≤", "beef", "th·ªãt b√≤ t∆∞∆°i", "b√≤"],
            "th·ªãt heo": ["th·ªãt heo", "pork", "th·ªãt l·ª£n", "heo"],
            "g√†": ["g√†", "chicken", "g√† ta", "g√† c√¥ng nghi·ªáp"],
            "t√¥m": ["t√¥m", "shrimp", "t√¥m s√∫", "t√¥m h√πm"],
            "rau c·∫£i": ["rau c·∫£i", "c·∫£i", "rau"],
            "khoai t√¢y": ["khoai t√¢y", "potato", "khoai"],
        }
        
        # üî• PERFORMANCE: Cache for entity extraction and normalization
        if Settings.ENABLE_AGENT_CACHE:
            self._extract_entity_cached = lru_cache(maxsize=Settings.AGENT_CACHE_SIZE)(self._extract_entity_impl)
            self._normalize_entity_cached = lru_cache(maxsize=Settings.AGENT_CACHE_SIZE)(self._normalize_entity_impl)
        else:
            self._extract_entity_cached = self._extract_entity_impl
            self._normalize_entity_cached = self._normalize_entity_impl
    
    async def process(self, state: Dict[str, Any]) -> Dict[str, Any]:
        """
        Resolve entity t·ª´ query
        """
        query = state.get("query", "").strip()
        sub_queries = state.get("sub_queries", {})
        product_query = sub_queries.get("product_search") or sub_queries.get("product_info") or query
        
        if not product_query:
            state["resolved_entity"] = None
            state["entity_normalized"] = None
            return state
        
        # üî• PERFORMANCE: Use cached extraction and normalization
        entity = self._extract_entity_cached(product_query)
        
        # Normalize entity (synonym mapping)
        normalized_entity = self._normalize_entity_cached(entity)
        
        # Validate entity c√≥ t·ªìn t·∫°i trong DB kh√¥ng (optional - c√≥ th·ªÉ skip n·∫øu SQL fail)
        entity_validated = await self._validate_entity_in_db(normalized_entity)
        
        state.update({
            "resolved_entity": entity,
            "entity_normalized": normalized_entity,
            "entity_validated": entity_validated,
            "entity_query": normalized_entity  # Query ƒë√£ ƒë∆∞·ª£c normalize ƒë·ªÉ search
        })
        
        self.log(f"‚úÖ Resolved entity: '{entity}' ‚Üí normalized: '{normalized_entity}' (validated: {entity_validated})")
        
        return state
    
    def _extract_entity_impl(self, query: str) -> str:
        """
        Extract entity (t√™n s·∫£n ph·∫©m) t·ª´ query
        """
        if not query:
            return ""
        
        query_lower = query.lower()
        
        # Lo·∫°i b·ªè stopwords
        stopwords = {
            "h√¨nh", "·∫£nh", "h√¨nh ·∫£nh", "l·∫•y", "ra", "v√†", "c·ªßa", "n√≥", "theo", "th√°ng",
            "doanh", "thu", "s·ªë", "th·ªëng", "k√™", "v·ªÅ", "v·ªõi", "cho", "t·ª´", "ƒë·∫øn",
            "s·∫£n", "ph·∫©m", "m√≥n", "b√°n", "mua", "t√¨m", "ki·∫øm"
        }
        
        query_clean = re.sub(r'[^\w\s]', ' ', query_lower)
        words = [w for w in query_clean.split() if w and w not in stopwords and len(w) > 2]
        
        # T√¨m c·ª•m t·ª´ ph·ªï bi·∫øn (2-3 t·ª´)
        if len(words) >= 2:
            # Th·ª≠ c·ª•m 2 t·ª´ tr∆∞·ªõc
            for i in range(len(words) - 1):
                phrase = f"{words[i]} {words[i+1]}"
                # Check n·∫øu phrase match v·ªõi synonym map
                for main_term, synonyms in self.synonym_map.items():
                    if phrase in main_term or main_term in phrase:
                        return main_term
                    if any(phrase in syn or syn in phrase for syn in synonyms):
                        return main_term
                # N·∫øu kh√¥ng match synonym, tr·∫£ v·ªÅ phrase
                if len(phrase) >= 4:
                    return phrase
        elif len(words) == 1:
            # Check synonym cho t·ª´ ƒë∆°n
            word = words[0]
            for main_term, synonyms in self.synonym_map.items():
                if word in main_term or main_term in word:
                    return main_term
                if word in synonyms:
                    return main_term
            return word
        
        # Fallback: tr·∫£ v·ªÅ t·ª´ ƒë·∫ßu ti√™n
        return words[0] if words else query.strip()
    
    def _normalize_entity_impl(self, entity: str) -> str:
        """
        Normalize entity b·∫±ng synonym map
        """
        if not entity:
            return entity
        
        entity_lower = entity.lower()
        
        # Check synonym map
        for main_term, synonyms in self.synonym_map.items():
            if entity_lower in main_term or main_term in entity_lower:
                return main_term
            if any(entity_lower in syn or syn in entity_lower for syn in synonyms):
                return main_term
        
        return entity
    
    async def _validate_entity_in_db(self, entity: str) -> bool:
        """
        Validate entity c√≥ t·ªìn t·∫°i trong DB kh√¥ng (optional check)
        N·∫øu SQL connection fail ‚Üí return True (assume valid ƒë·ªÉ ti·∫øp t·ª•c)
        """
        if not entity:
            return False
        
        try:
            connection_string = Settings.DATABASE_CONNECTION_STRING
            if not connection_string:
                # Kh√¥ng c√≥ connection string ‚Üí assume valid
                return True
            
            import pyodbc
            import asyncio
            
            def check_in_db():
                conn = None
                try:
                    conn = pyodbc.connect(connection_string)
                    cursor = conn.cursor()
                    
                    # Quick check: c√≥ s·∫£n ph·∫©m n√†o match kh√¥ng
                    like_pattern = f"%{entity}%"
                    query = """
                        SELECT TOP 1 MaSanPham
                        FROM SanPham
                        WHERE (IsDeleted = 0 OR IsDeleted IS NULL)
                          AND TenSanPham LIKE ?
                    """
                    cursor.execute(query, like_pattern)
                    row = cursor.fetchone()
                    cursor.close()
                    return row is not None
                except Exception as e:
                    logger.warning(f"Error validating entity in DB: {str(e)}")
                    return True  # Assume valid n·∫øu SQL fail
                finally:
                    if conn:
                        conn.close()
            
            result = await asyncio.to_thread(check_in_db)
            return result
            
        except Exception as e:
            logger.warning(f"Error in entity validation: {str(e)}")
            return True  # Assume valid ƒë·ªÉ ti·∫øp t·ª•c

